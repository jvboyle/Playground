{{- if .Values.prometheus.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: prometheus-rules
  namespace: {{ .Release.Namespace }}
data:
  kube-state-metrics.rules.yml: |+
    groups:
    - name: kube-state-metrics.rules
      rules:
      - alert: DeploymentGenerationMismatch
        expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Observed deployment generation does not match expected one for
            deployment {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.deployment{{ "}}" }}
          summary: "Cluster: {{ .Values.k8Cluster }} Deployment is outdated"
      - alert: DeploymentReplicasNotUpdated
        expr: ((kube_deployment_status_replicas_updated != kube_deployment_spec_replicas)
          or (kube_deployment_status_replicas_available != kube_deployment_spec_replicas))
          unless (kube_deployment_spec_paused == 1)
        for: 15m
        labels:
          severity: critical
        annotations:
          description: 'A Pod did not start successfully\nNamespace:{{ "{{" }}$labels.namespace{{ "}}" }}\nDeployment:{{ "{{" }}$labels.deployment{{ "}}" }}\nJob:{{ "{{" }}$labels.job{{ "}}" }}'
          summary: "Cluster: {{ .Values.k8Cluster }} A Pod did not start successfully"
      - alert: DaemonSetRolloutStuck
        expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled
          * 100 < 100
        for: 15m
        labels:
          severity: warning
        annotations:
          description: 'Only {{ "{{" }}$value{{ "}}" }}% of desired pods scheduled and ready for daemon
            set {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.daemonset{{ "}}" }}'
          summary: "Cluster: {{ .Values.k8Cluster }} DaemonSet is missing pods"
      - alert: K8SDaemonSetsNotScheduled
        expr: kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled
          > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: A number of daemonsets are not scheduled.
          summary: "Cluster: {{ .Values.k8Cluster }} Daemonsets are not scheduled correctly"
      - alert: DaemonSetsMissScheduled
        expr: kube_daemonset_status_number_misscheduled > 0
        for: 10m
        labels:
          severity: critical
        annotations:
          description: A number of daemonsets are running where they are not supposed to run.
          summary: "Cluster: {{ .Values.k8Cluster }} Daemonsets are not scheduled correctly"
      - alert: PodFrequentlyRestarting
        expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
        for: 10m
        labels:
          severity: warning
        annotations:
          description: 'Namespace: {{ "{{" }}$labels.namespace{{ "}}" }}\nInstance: {{ "{{" }}$labels.instance{{ "}}" }}\nPod: {{ "{{" }}$labels.pod{{ "}}" }}\nPod has restarted {{ "{{" }}$value{{ "}}" }} times within the last hour.'
          summary: "Cluster: {{ .Values.k8Cluster }} a pod has restarted more than 5 times in the past hour."
  node.rules.yml: |+
    groups:
    - name: node.rules
      rules:
      - record: instance:node_cpu:rate:sum
        expr: sum(rate(node_cpu{mode!="idle",mode!="iowait",mode!~"^(?:guest.*)$"}[3m]))
          BY (instance)
      - record: instance:node_filesystem_usage:sum
        expr: sum((node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"}))
          BY (instance)
      - record: instance:node_network_receive_bytes:rate:sum
        expr: sum(rate(node_network_receive_bytes[3m])) BY (instance)
      - record: instance:node_network_transmit_bytes:rate:sum
        expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)
      - record: instance:node_cpu:ratio
        expr: sum(rate(node_cpu{mode!="idle"}[5m])) WITHOUT (cpu, mode) / ON(instance)
          GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)
      - record: cluster:node_cpu:sum_rate5m
        expr: sum(rate(node_cpu{mode!="idle"}[5m]))
      - record: cluster:node_cpu:ratio
        expr: cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))
      - alert: NodeDiskRunningFull
        expr: 100 *(1 - (node_filesystem_free{ mountpoint="/"} / node_filesystem_size{ mountpoint="/"})) > 80
        for: 5m
        labels:
          severity: critical
        annotations:
          description: '{{ "{{" }}$labels.instance{{ "}}" }}Disk Usage is above 80% (current value is: {{ "{{" }}$value{{ "}}" }})'
          summary: "Cluster: {{ .Values.k8Cluster }}"
      - alert: NodeCPUUsage
        expr: 100 - (avg by (instance) (irate(node_cpu{name="node-exporter",mode="idle"}[5m])) * 100) > 90
        for: 10m
        labels:
          severity: critical
        annotations:
          description: '{{ "{{" }}$labels.instance{{ "}}" }}: CPU usage is above 90% (current value is: {{ "{{" }}$value{{ "}}" }})'
          summary: "Cluster: {{ .Values.k8Cluster }}"
      - alert: iNodeUsage
        expr: sum(container_fs_inodes_free) by (device) == 0
        for: 15m
        labels:
          severity: critical
        annotations:
          description: '{{ "{{" }}$labels.instance{{ "}}" }}: iNodes is Full'
          summary: "Cluster: {{ .Values.k8Cluster }}"
      - alert: FileSystemReadOnly
        expr: node_filesystem_readonly{mountpoint="/"} == 1
        for: 10m
        labels:
          severity: critical
        annotations:
          description: '{{ "{{" }}$labels.instance{{ "}}" }}: FileSystem is Read-Only'
          summary: "Cluster: {{ .Values.k8Cluster }}"
  general.rules.yml: |+
    groups:
    - name: general.rules
      rules:
      - alert: TargetDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 15m
        labels:
          severity: critical
        annotations:
          description: 'Instance: {{ "{{" }}$labels.instance{{ "}}" }}\nJob: {{ "{{" }}$labels.job{{ "}}" }}\n'
          summary: 'Cluster: {{ .Values.k8Cluster }}\nInstance: {{ "{{" }}$labels.instance{{ "}}" }} has been down for 15 minutes.'
      - record: fd_utilization
        expr: process_open_fds / process_max_fds
      - alert: FdExhaustionClose
        expr: predict_linear(fd_utilization[1h], 3600 * 4) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: '{{ "{{" }}$labels.job{{ "}}" }}: {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} instance {{ "{{" }} $labels.instance {{ "}}" }}
            will exhaust in file/socket descriptors within the next 4 hours'
          summary: "Cluster: {{ .Values.k8Cluster }} file descriptors soon exhausted"
      - alert: FdExhaustionClose
        expr: predict_linear(fd_utilization[10m], 3600) > 1
        for: 10m
        labels:
          severity: critical
        annotations:
          description: '{{ "{{" }}$labels.job{{ "}}" }}: {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} instance {{ "{{" }} $labels.instance {{ "}}" }}
            will exhaust in file/socket descriptors within the next hour'
          summary: "Cluster: {{ .Values.k8Cluster }} file descriptors soon exhausted"
{{- end }}
